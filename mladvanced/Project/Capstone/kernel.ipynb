{
  "cells": [
    {
      "metadata": {
        "_uuid": "dde6d881ba36ed71da0b4f37d60e45c65feb761b"
      },
      "cell_type": "markdown",
      "source": "# Steps\nThis kernel is designed in following ways:\n\n**[Step 1](#step1)**: Data Preprocessing\n\n**[Step 2](#step2)**:  Develop a Benchmark model\n\n**[Step 3](#step3)**: Develop a CNN architecture from scratch\n\n**[Step 4](#step4)**: Develop a CNN using Transfer Learning\n"
    },
    {
      "metadata": {
        "_uuid": "10bfeff740d98e3cf25d2bc930064dc52517ef26"
      },
      "cell_type": "markdown",
      "source": "---\n<a id='step1'></a>\n## Step 1: Data Preprocessing\n\n### Import Libraries\nHere we import a set of useful libraries"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom glob import glob\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.datasets import load_files \nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\n\nfrom keras.utils import np_utils\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/flowers/flowers\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9565a2377b008ef92af453f92b3276ebccb13656"
      },
      "cell_type": "markdown",
      "source": "### Reorganize the data\nAll the flowers are stored in a directory flower and separated based on the category in sub-directory.\nWe can reorganize the data in such a way that we can easily use `load_files` from `sklearn`."
    },
    {
      "metadata": {
        "_uuid": "547cc0ee6358df4cb638e256151d8a5e0c8abf75"
      },
      "cell_type": "markdown",
      "source": "The flowers are present in dataset as follows: \n\n```\nflowers\n│\n└───Daisy\n│   \n└───Dandelion\n|\n└───Rose\n│   \n└───Sunflower\n|\n└───Tulip\n```\n\nWe can create dataset for training, validation and testing to easily use `load_files` from `sklearn`\n\n```\ndata\n│\n└───train\n|    │\n|    └───Daisy\n|    │   \n|    └───Dandelion\n|    |\n|    └───Rose\n|    │   \n|    └───Sunflower\n|    |\n|    └───Tulip\n└───valid\n|    │\n|    └───Daisy\n|    │   \n|    └───Dandelion\n|    |\n|    └───Rose\n|    │   \n|    └───Sunflower\n|    |\n|    └───Tulip\n└───test\n     │\n     └───Daisy\n     │   \n     └───Dandelion\n     |\n     └───Rose\n     │   \n     └───Sunflower\n     |\n     └───Tulip\n```\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e81249e92a5414aad0e2e1afb5aeb293151d376c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Make a parent directory `data` and three sub directories `train`, `valid` and 'test'\n%rm -rf data # Remove if already present\n\n%mkdir -p data/train/daisy\n%mkdir -p data/train/tulip\n%mkdir -p data/train/sunflower\n%mkdir -p data/train/rose\n%mkdir -p data/train/dandelion\n\n%mkdir -p data/valid/daisy\n%mkdir -p data/valid/tulip\n%mkdir -p data/valid/sunflower\n%mkdir -p data/valid/rose\n%mkdir -p data/valid/dandelion\n\n%mkdir -p data/test/daisy\n%mkdir -p data/test/tulip\n%mkdir -p data/test/sunflower\n%mkdir -p data/test/rose\n%mkdir -p data/test/dandelion\n\n\n%ls data/train\n%ls data/valid\n%ls data/test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2d0865ccd43644ef0dd7c777b33d7b699f29fa8b"
      },
      "cell_type": "markdown",
      "source": "Find all the categories of the flowers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b5811016db3f6cad358cd2f5f556d69e3aee9cd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "base_dir = \"../input/flowers/flowers\"\ncategories = os.listdir(base_dir)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "934ce235a1c6530e8851ce6c886326613ac7e678"
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random\nfrom shutil import copyfile\n\nplt.rcParams[\"figure.figsize\"] = (20,3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a05b3d6811b1507423499abe360e7804cd36e5b0"
      },
      "cell_type": "code",
      "source": "def train_valid_test(files):\n    \"\"\"This function splits the files in training, validation and testing sets with 60%, 20%\n    and 20% of data in each respectively\"\"\"\n    train_fles = files[:int(len(files)*0.6)]\n    valid_files = files[int(len(files)*0.6):int(len(files)*0.8)]\n    test_files = files[int(len(files)*0.8):]\n    return train_fles, valid_files, test_files",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d783145250c99d2ea63fdd64c575ca7aec69517e"
      },
      "cell_type": "code",
      "source": "def copy_files(files, src, dest):\n    \"\"\"This function copy files from src to dest\"\"\"\n    for file in files:\n        copyfile(\"{}/{}\".format(src, file), \"{}/{}\".format(dest, file))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "7c4fc45bc1233bd36d716238459f250ff43f22b3"
      },
      "cell_type": "code",
      "source": "def plot_images(category, images):\n    \"\"\"This method plots five images from a category\"\"\"\n    for i in range(len(images)):\n        plt.subplot(1,5,i+1)\n        plt.title(category)\n        image = mpimg.imread(\"{}/{}/{}\".format(base_dir, category, images[i]))\n        plt.imshow(image)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7eec258952c31dde6dd4c3f24471a013e8873d96",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "total_images = []\nfor category in categories:\n    images = os.listdir(\"{}/{}\".format(base_dir, category))\n    random.shuffle(images)\n    filtered_images = [image for image in images if image not in ['flickr.py', 'flickr.pyc', 'run_me.py']]\n    \n    total_images.append(len(filtered_images))\n    \n    \n    train_images, valid_images, test_images = train_valid_test(filtered_images)\n    \n    copy_files(train_images, \"{}/{}\".format(base_dir, category), \"./data/train/{}\".format(category))\n    copy_files(valid_images, \"{}/{}\".format(base_dir, category), \"./data/valid/{}\".format(category))\n    copy_files(test_images, \"{}/{}\".format(base_dir, category), \"./data/test/{}\".format(category))\n    plot_images(category, images[:5])\n    \n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "324a6974c88620ccfabf62d0c10b1a2ce29f8fc8"
      },
      "cell_type": "markdown",
      "source": "### Statistics of flowers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0c7bbd703b410f358cb37ae23149765425a97ef",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"Total images: {}\".format(np.sum(total_images)))\nfor i in range(len(categories)):\n    print(\"{}: {}\".format(categories[i], total_images[i]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b42170d0f39c7da07ef0dc9cae08ffdaf400e7e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "y_pos = np.arange(len(categories))\nplt.bar(y_pos, total_images, width=0.2,color='b',align='center')\nplt.xticks(y_pos, categories)\nplt.ylabel(\"Image count\")\nplt.title(\"Image count in different categories\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d4dbfd8d52917e2170845c51df21dc37f2bb3a1b"
      },
      "cell_type": "markdown",
      "source": "### Observations\n- There are 4323 total images with approximately similar distribution in each category.\n- The dataset does not seem  to be imbalanced.\n- Accuracy can be used as a metric for model evaulation."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# define function to load train, valid and test datasets\ndef load_dataset(path):\n    data = load_files(path)\n    flower_files = np.array(data['filenames'])\n    print(data['target_names'])\n    flower_targets = np_utils.to_categorical(np.array(data['target']), 5)\n    return flower_files, flower_targets\n\n# load train, test, and validation datasets\ntrain_files, train_targets = load_dataset('data/train')\nvalid_files, valid_targets = load_dataset('data/valid')\ntest_files, test_targets = load_dataset('data/test')\n\nprint('There are %d total flower categories.' % len(categories))\nprint('There are %s total flower images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\nprint('There are %d training flower images.' % len(train_files))\nprint('There are %d validation flower images.' % len(valid_files))\nprint('There are %d test flower images.' % len(test_files))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "df43f27877826f8c21323fe03d69d19198f13950"
      },
      "cell_type": "markdown",
      "source": "### Data Transformation\n\nKeras' CNNs require a 4D tensor as input with the shape as `(nb_samples, rows, columns, channels)` where\n- `nb_samples`: total number of samples or images\n- `rows`: number of rows of each image\n- `columns`: number of columns of each image\n- `channels`: number of channels of each image\n"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "acbac7aa1dd0dbaa52c61a8755c57e39aedadbba"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing import image                  \nfrom tqdm import tqdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f3a00ca03ac3aada32a4d272add360b61f8e473e"
      },
      "cell_type": "markdown",
      "source": "### Create a 4D tensor\nThe `path_to_tensor` function below takes a color image as input and returns a 4D tensor suitable for supplying to Keras CNN. The function first loads the image and then resizes it 224x224 pixels. The image then, is converted to an array and resized to a 4D tensor. The returned tensor will always have a shape of `(1, 224, 224, 3)` as we are dealing with a single image only in this function."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "bca267ceb6ffbd93ef9ada538eddf4034438bc86"
      },
      "cell_type": "code",
      "source": "def path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "000c9b35b3b279d3ecb3afbda2a4cf6718a1c7e4"
      },
      "cell_type": "markdown",
      "source": "The `ptahs_to_tensor` applies `path_to_tensor` to all images and returns a list of tensors."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "4adb0b529243b4ef0aa18fd5ed925ac2ba611bba"
      },
      "cell_type": "code",
      "source": "def paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88aa64593b43b1db1cfd46a55d74d59148eef469"
      },
      "cell_type": "markdown",
      "source": "### Pre-process the Data\nRescale the images by dividing every pixel in every image by 255."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5b33687d2417022371a1e224477c0d292ebd52f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')/255\ntest_tensors = paths_to_tensor(test_files).astype('float32')/255",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72e8ecf4036d248db942b92b2fc92b06a461b6ab"
      },
      "cell_type": "markdown",
      "source": "<a id=\"step2\"></a>\n## Step 2: Develop a Benchmark model\nUse a simple CNN to create a benchmark model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "046e0feba1295afcb7f6d43a150ee9ad2a6fbfe9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "simple_model = Sequential()\nprint(train_tensors.shape)\n\n### Define the architecture of the simple model.\nsimple_model.add(Conv2D(filters=16, kernel_size=2, strides=1, activation='relu', input_shape=(224,224,3)))\nsimple_model.add(GlobalAveragePooling2D())\nsimple_model.add(Dense(5, activation='softmax'))\nsimple_model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2855aca7c4124163183fc2e038c64f0e752214a9"
      },
      "cell_type": "markdown",
      "source": "### Making Predictions with the simple model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "373943475e2222afbe46654fcc1651209c803ae5"
      },
      "cell_type": "code",
      "source": "simple_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "85386373dbc9acb029b14b3861314171a87518f5"
      },
      "cell_type": "code",
      "source": "# Create a `saved_models` directory for saving best model\n%mkdir -p saved_models",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b58f8f7edc1b6d460087ae581bfecd5711ed9300",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.callbacks import ModelCheckpoint  \n\n### number of epochs\nepochs = 50\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.simple.hdf5', \n                               verbose=1, save_best_only=True)\n\nsimple_model.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49b610f7a36c8680f43e7a179885d54fbcaf034c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "simple_model.load_weights('saved_models/weights.best.simple.hdf5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f391f468bb616523eef09bdc76e15b1762ce5c93",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# get index of predicted flower category for each image in test set\nflower_predictions = [np.argmax(simple_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(flower_predictions)==np.argmax(test_targets, axis=1))/len(flower_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d542d814259cb6f15c4b310b74f160eb9edba7a"
      },
      "cell_type": "markdown",
      "source": "### Benchmark model's performance\nThe accuracy obtained from the benchmark model is 41.57%."
    },
    {
      "metadata": {
        "_uuid": "04f73c9285a58e8fc6cd18c7ad973d2506e86993"
      },
      "cell_type": "markdown",
      "source": "---\n<a id=\"step3\"></a>\n## Step 3: Develop a CNN architecture from scratch"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e4ec8d9cc08cd48cceeb6ac69a9caf02d43a365",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\nprint(train_tensors.shape)\n### Define architecture.\nmodel.add(Conv2D(filters=16, kernel_size=2, strides=1, activation='relu', input_shape=(224,224,3)))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=32, kernel_size=2, strides=1, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=64, kernel_size=2, strides=1, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(5, activation='softmax'))\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6751a015f8613138588718779e04993b454fb063"
      },
      "cell_type": "code",
      "source": "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d050838f5bd4cecc32a79aaaea3260f0e9a1a615",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.callbacks import ModelCheckpoint  \n\n### number of epochs\nepochs = 50\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\nmodel.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7bda01aa5709d494bac2a39064df01479a93883e"
      },
      "cell_type": "markdown",
      "source": "#### Load best weight of the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09a05573b5ced031299ee3e57fa5e03796c14587",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model.load_weights('saved_models/weights.best.from_scratch.hdf5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8109a4d1fc802946ec1995c896f8bfe821acd881"
      },
      "cell_type": "markdown",
      "source": "#### Get the accuracy of the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9802b29568832805373da6657fae589230f29378",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# get index of predicted flower category for each image in test set\nflower_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(flower_predictions)==np.argmax(test_targets, axis=1))/len(flower_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96fc5206f80a9ed8d6ffb8a33ccce4105bb1bd88"
      },
      "cell_type": "markdown",
      "source": "<a id=\"step4\"></a>\n## Step 4: Develop a CNN using Transfer Learning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42befd553d26594626394fed47b989d64e91a4c6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model\n\ninception_resnet = InceptionResNetV2(weights=\"imagenet\",include_top=False, input_shape=(224,224,3))\nfor layer in inception_resnet.layers[:5]:\n    layer.trainable = False\n\noutput_model = inception_resnet.output\noutput_model = Flatten()(output_model)\noutput_model = Dense(200, activation='relu')(output_model)\noutput_model = Dropout(0.5)(output_model)\noutput_model = Dense(200, activation='relu')(output_model)\noutput_model = Dense(5, activation='softmax')(output_model)\n\nmodel = Model(inputs=inception_resnet.input, outputs=output_model)\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0499a596daa9a617c5b377a85319649cc4d97eeb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7647bf155e9b8e2bb5862c238b2c1dd677939f69",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.callbacks import ModelCheckpoint  \n\n### number of epochs\nepochs = 50\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.inception_resnetv2.hdf5', \n                               verbose=1, save_best_only=True)\n\nmodel.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d6c6c3954bc407157a3cd652c9effd186282b2b3"
      },
      "cell_type": "markdown",
      "source": "#### Load the best weight of the model"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "71eb486ede3dcf2e701fae8667bc8a939a703d55"
      },
      "cell_type": "code",
      "source": "### load best weights\nmodel.load_weights('saved_models/weights.best.inception_resnetv2.hdf5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "51100dc3587f9961ad1c26a778a65249c3fc36e9"
      },
      "cell_type": "markdown",
      "source": "#### Get the accuracy on test set"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec4f83cd5c9936e727959ecccb74468de8d549a8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# get index of predicted flower category for each image in test set \nflower_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(flower_predictions)==np.argmax(test_targets, axis=1))/len(flower_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d46091ba795968bcc8081d44c1bdce498c77ce3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for i in range(5):\n    predicted = np.argmax(model.predict(np.expand_dims(test_tensors[i], axis=0)))\n    actual = np.argmax(test_targets[i])\n    print(\"Predicted: {}, Actual: {}, Name: {}\".format(predicted, actual, test_files[i].split(\"/\")[2]))\n    image = mpimg.imread(test_files[i])\n    plt.imshow(image)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9457aadd205b35b5522a3012c008a1288c9a737e"
      },
      "cell_type": "markdown",
      "source": "Delete created directory and files. It's necessary to have only few files otherwise Kaggle won't allow to commit a kernel."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a28b6b935bc98c4cf36a9a46d6f23826130e7fcd"
      },
      "cell_type": "code",
      "source": "%rm -rf data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d585ca12f529fe366ddf2f4c0bb2abd1893bf154"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}